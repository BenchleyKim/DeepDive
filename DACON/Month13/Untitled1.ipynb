{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.7.1  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "DEVICE = None\n",
    "\n",
    "if torch.cuda.is_available() :\n",
    "    DEVICE = torch.device('cuda')\n",
    "else : \n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n",
      "42\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "\n",
    "from glob import glob\n",
    "data_path = './data/train/'\n",
    "file_list = glob(data_path+'*.npy')\n",
    "file_list.sort()\n",
    "\n",
    "\n",
    "traindataset = []\n",
    "for i in range(len(file_list)) :\n",
    "    file = np.load(file_list[i])\n",
    "    file = np.float32(file)\n",
    "    file = torch.from_numpy(file) \n",
    "    traindataset.append(file)\n",
    "testdataset = traindataset[440:]\n",
    "traindataset = traindataset[:440]\n",
    "print(len(traindataset))\n",
    "print(len(testdataset))\n",
    "print(traindataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-be0523a7c1d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mASICDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\" 해빙 데이터 \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilelist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatalist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilelist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class ASICDataset(Dataset) :\n",
    "    \"\"\" 해빙 데이터 \"\"\"\n",
    "    def __init__(self, filelist, root_dir, transform = None) :\n",
    "        self.datalist = filelist\n",
    "        self.root_dir = root_dir\n",
    "        self.trainsform = transform\n",
    "    def __len__(self) :\n",
    "        return len(self.datalist)\n",
    "    def __getitem__(self,idx) :\n",
    "        if torch.is_tensor(idx) :\n",
    "            idx = idx.tolist()\n",
    "        file = np.load(self.datalist[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=136192, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=32, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=136192, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 1139.028198\n",
      "Train Epoch: 1 [1000/60000 (11%)]\tTrain Loss: 654.053650\n",
      "Train Epoch: 1 [2000/60000 (21%)]\tTrain Loss: 1809.005493\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 4.3291\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 1132.220093\n",
      "Train Epoch: 2 [1000/60000 (11%)]\tTrain Loss: 646.205811\n",
      "Train Epoch: 2 [2000/60000 (21%)]\tTrain Loss: 1791.950073\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 4.2788\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 1119.199219\n",
      "Train Epoch: 3 [1000/60000 (11%)]\tTrain Loss: 635.564453\n",
      "Train Epoch: 3 [2000/60000 (21%)]\tTrain Loss: 1770.432373\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 4.2174\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 1103.016724\n",
      "Train Epoch: 4 [1000/60000 (11%)]\tTrain Loss: 623.503357\n",
      "Train Epoch: 4 [2000/60000 (21%)]\tTrain Loss: 1746.124023\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 4.1500\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 1084.989380\n",
      "Train Epoch: 5 [1000/60000 (11%)]\tTrain Loss: 610.963257\n",
      "Train Epoch: 5 [2000/60000 (21%)]\tTrain Loss: 1720.305054\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 4.0808\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 1066.149658\n",
      "Train Epoch: 6 [1000/60000 (11%)]\tTrain Loss: 598.698242\n",
      "Train Epoch: 6 [2000/60000 (21%)]\tTrain Loss: 1694.065796\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 4.0131\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 1047.365845\n",
      "Train Epoch: 7 [1000/60000 (11%)]\tTrain Loss: 587.308289\n",
      "Train Epoch: 7 [2000/60000 (21%)]\tTrain Loss: 1668.337769\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 3.9494\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 1029.352051\n",
      "Train Epoch: 8 [1000/60000 (11%)]\tTrain Loss: 577.234680\n",
      "Train Epoch: 8 [2000/60000 (21%)]\tTrain Loss: 1643.883057\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 3.8919\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 1012.656189\n",
      "Train Epoch: 9 [1000/60000 (11%)]\tTrain Loss: 568.755737\n",
      "Train Epoch: 9 [2000/60000 (21%)]\tTrain Loss: 1621.280640\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 3.8417\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 997.654358\n",
      "Train Epoch: 10 [1000/60000 (11%)]\tTrain Loss: 561.994446\n",
      "Train Epoch: 10 [2000/60000 (21%)]\tTrain Loss: 1600.923706\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 3.7995\n",
      "Train Epoch: 11 [0/60000 (0%)]\tTrain Loss: 984.559143\n",
      "Train Epoch: 11 [1000/60000 (11%)]\tTrain Loss: 556.939636\n",
      "Train Epoch: 11 [2000/60000 (21%)]\tTrain Loss: 1583.029907\n",
      "\n",
      "[EPOCH: 11], \tTest Loss: 3.7653\n",
      "Train Epoch: 12 [0/60000 (0%)]\tTrain Loss: 973.440796\n",
      "Train Epoch: 12 [1000/60000 (11%)]\tTrain Loss: 553.475281\n",
      "Train Epoch: 12 [2000/60000 (21%)]\tTrain Loss: 1567.663452\n",
      "\n",
      "[EPOCH: 12], \tTest Loss: 3.7387\n",
      "Train Epoch: 13 [0/60000 (0%)]\tTrain Loss: 964.256042\n",
      "Train Epoch: 13 [1000/60000 (11%)]\tTrain Loss: 551.412903\n",
      "Train Epoch: 13 [2000/60000 (21%)]\tTrain Loss: 1554.761475\n",
      "\n",
      "[EPOCH: 13], \tTest Loss: 3.7189\n",
      "Train Epoch: 14 [0/60000 (0%)]\tTrain Loss: 956.878784\n",
      "Train Epoch: 14 [1000/60000 (11%)]\tTrain Loss: 550.520813\n",
      "Train Epoch: 14 [2000/60000 (21%)]\tTrain Loss: 1544.165527\n",
      "\n",
      "[EPOCH: 14], \tTest Loss: 3.7051\n",
      "Train Epoch: 15 [0/60000 (0%)]\tTrain Loss: 951.127808\n",
      "Train Epoch: 15 [1000/60000 (11%)]\tTrain Loss: 550.549377\n",
      "Train Epoch: 15 [2000/60000 (21%)]\tTrain Loss: 1535.649780\n",
      "\n",
      "[EPOCH: 15], \tTest Loss: 3.6962\n",
      "Train Epoch: 16 [0/60000 (0%)]\tTrain Loss: 946.788757\n",
      "Train Epoch: 16 [1000/60000 (11%)]\tTrain Loss: 551.249084\n",
      "Train Epoch: 16 [2000/60000 (21%)]\tTrain Loss: 1528.948486\n",
      "\n",
      "[EPOCH: 16], \tTest Loss: 3.6910\n",
      "Train Epoch: 17 [0/60000 (0%)]\tTrain Loss: 943.631409\n",
      "Train Epoch: 17 [1000/60000 (11%)]\tTrain Loss: 552.384949\n",
      "Train Epoch: 17 [2000/60000 (21%)]\tTrain Loss: 1523.778809\n",
      "\n",
      "[EPOCH: 17], \tTest Loss: 3.6887\n",
      "Train Epoch: 18 [0/60000 (0%)]\tTrain Loss: 941.424561\n",
      "Train Epoch: 18 [1000/60000 (11%)]\tTrain Loss: 553.748047\n",
      "Train Epoch: 18 [2000/60000 (21%)]\tTrain Loss: 1519.859985\n",
      "\n",
      "[EPOCH: 18], \tTest Loss: 3.6882\n",
      "Train Epoch: 19 [0/60000 (0%)]\tTrain Loss: 939.947205\n",
      "Train Epoch: 19 [1000/60000 (11%)]\tTrain Loss: 555.165161\n",
      "Train Epoch: 19 [2000/60000 (21%)]\tTrain Loss: 1516.929565\n",
      "\n",
      "[EPOCH: 19], \tTest Loss: 3.6887\n",
      "Train Epoch: 20 [0/60000 (0%)]\tTrain Loss: 939.001343\n",
      "Train Epoch: 20 [1000/60000 (11%)]\tTrain Loss: 556.506958\n",
      "Train Epoch: 20 [2000/60000 (21%)]\tTrain Loss: 1514.756348\n",
      "\n",
      "[EPOCH: 20], \tTest Loss: 3.6898\n",
      "Train Epoch: 21 [0/60000 (0%)]\tTrain Loss: 938.420471\n",
      "Train Epoch: 21 [1000/60000 (11%)]\tTrain Loss: 557.692444\n",
      "Train Epoch: 21 [2000/60000 (21%)]\tTrain Loss: 1513.148560\n",
      "\n",
      "[EPOCH: 21], \tTest Loss: 3.6910\n",
      "Train Epoch: 22 [0/60000 (0%)]\tTrain Loss: 938.076538\n",
      "Train Epoch: 22 [1000/60000 (11%)]\tTrain Loss: 558.686279\n",
      "Train Epoch: 22 [2000/60000 (21%)]\tTrain Loss: 1511.954712\n",
      "\n",
      "[EPOCH: 22], \tTest Loss: 3.6921\n",
      "Train Epoch: 23 [0/60000 (0%)]\tTrain Loss: 937.878845\n",
      "Train Epoch: 23 [1000/60000 (11%)]\tTrain Loss: 559.488831\n",
      "Train Epoch: 23 [2000/60000 (21%)]\tTrain Loss: 1511.062378\n",
      "\n",
      "[EPOCH: 23], \tTest Loss: 3.6930\n",
      "Train Epoch: 24 [0/60000 (0%)]\tTrain Loss: 937.768921\n",
      "Train Epoch: 24 [1000/60000 (11%)]\tTrain Loss: 560.122437\n",
      "Train Epoch: 24 [2000/60000 (21%)]\tTrain Loss: 1510.389648\n",
      "\n",
      "[EPOCH: 24], \tTest Loss: 3.6937\n",
      "Train Epoch: 25 [0/60000 (0%)]\tTrain Loss: 937.710938\n",
      "Train Epoch: 25 [1000/60000 (11%)]\tTrain Loss: 560.617126\n",
      "Train Epoch: 25 [2000/60000 (21%)]\tTrain Loss: 1509.877686\n",
      "\n",
      "[EPOCH: 25], \tTest Loss: 3.6942\n",
      "Train Epoch: 26 [0/60000 (0%)]\tTrain Loss: 937.683655\n",
      "Train Epoch: 26 [1000/60000 (11%)]\tTrain Loss: 561.002197\n",
      "Train Epoch: 26 [2000/60000 (21%)]\tTrain Loss: 1509.484985\n",
      "\n",
      "[EPOCH: 26], \tTest Loss: 3.6947\n",
      "Train Epoch: 27 [0/60000 (0%)]\tTrain Loss: 937.674438\n",
      "Train Epoch: 27 [1000/60000 (11%)]\tTrain Loss: 561.302185\n",
      "Train Epoch: 27 [2000/60000 (21%)]\tTrain Loss: 1509.181641\n",
      "\n",
      "[EPOCH: 27], \tTest Loss: 3.6950\n",
      "Train Epoch: 28 [0/60000 (0%)]\tTrain Loss: 937.675598\n",
      "Train Epoch: 28 [1000/60000 (11%)]\tTrain Loss: 561.536499\n",
      "Train Epoch: 28 [2000/60000 (21%)]\tTrain Loss: 1508.946167\n",
      "\n",
      "[EPOCH: 28], \tTest Loss: 3.6952\n",
      "Train Epoch: 29 [0/60000 (0%)]\tTrain Loss: 937.682434\n",
      "Train Epoch: 29 [1000/60000 (11%)]\tTrain Loss: 561.719849\n",
      "Train Epoch: 29 [2000/60000 (21%)]\tTrain Loss: 1508.762207\n",
      "\n",
      "[EPOCH: 29], \tTest Loss: 3.6954\n",
      "Train Epoch: 30 [0/60000 (0%)]\tTrain Loss: 937.691956\n",
      "Train Epoch: 30 [1000/60000 (11%)]\tTrain Loss: 561.863464\n",
      "Train Epoch: 30 [2000/60000 (21%)]\tTrain Loss: 1508.618042\n",
      "\n",
      "[EPOCH: 30], \tTest Loss: 3.6955\n"
     ]
    }
   ],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(136192,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,32)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,136192)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "model = AE().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "def train(model, traindataset, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, image in enumerate(traindataset):\n",
    "        image = image.view(-1,136192).to(DEVICE)\n",
    "        target = image.view(-1,136192).to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        encoded, decoded = model(image)\n",
    "        loss = criterion(decoded, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0 :\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "def evalutate(model, testdataset) :\n",
    "    model.eval()\n",
    "    test_loss = 0 \n",
    "    real_image = []\n",
    "    gen_image = []\n",
    "    with torch.no_grad():\n",
    "        for image  in testdataset :\n",
    "            image = image.view(-1,136192).to(DEVICE)\n",
    "            target = image.view(-1,136192).to(DEVICE)\n",
    "            encoded, decoded = model(image)\n",
    "\n",
    "            test_loss += criterion(decoded,image).item()\n",
    "            real_image.append(image.to(\"cpu\"))\n",
    "            gen_image.append(decoded.to(\"cpu\"))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return test_loss, real_image, gen_image\n",
    "\n",
    "\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    train(model, traindataset, optimizer, log_interval = 200)\n",
    "    test_loss, real_image, gen_image = evalutate(model, testdataset)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}\".format(epoch, test_loss))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
